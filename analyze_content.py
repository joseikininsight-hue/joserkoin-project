#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ©æˆé‡‘ãƒ»è£œåŠ©é‡‘æŠ•ç¨¿ã®Cåˆ—æœ¬æ–‡ã‚’åˆ†æã—ã€SEOå¯¾ç­–ã®å•é¡Œç‚¹ã‚’æ´—ã„å‡ºã™
"""

import openpyxl
from bs4 import BeautifulSoup
import re
from collections import Counter
import json

def analyze_html_structure(html_content):
    """HTMLæ§‹é€ ã‚’åˆ†æ"""
    if not html_content or not isinstance(html_content, str):
        return None
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    analysis = {
        'has_content': bool(html_content.strip()),
        'length': len(html_content),
        'has_html_tags': bool(soup.find()),
        'has_semantic_tags': False,
        'heading_structure': [],
        'css_classes': [],
        'inline_styles': [],
        'semantic_tags': [],
        'links_count': 0,
        'images_count': 0,
        'tables_count': 0,
        'lists_count': 0,
        'divs_count': 0,
        'sections_count': 0,
        'has_schema': False,
        'seo_issues': []
    }
    
    # è¦‹å‡ºã—æ§‹é€ ã®ç¢ºèª
    for level in range(1, 7):
        headings = soup.find_all(f'h{level}')
        if headings:
            analysis['heading_structure'].append({
                'level': level,
                'count': len(headings),
                'texts': [h.get_text()[:50] for h in headings[:3]]  # æœ€åˆã®3ã¤ã®ã¿
            })
    
    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¿ã‚°ã®ç¢ºèª
    semantic_tags = ['article', 'section', 'header', 'footer', 'nav', 'aside', 'main']
    for tag in semantic_tags:
        if soup.find(tag):
            analysis['semantic_tags'].append(tag)
            analysis['has_semantic_tags'] = True
    
    # CSSã‚¯ãƒ©ã‚¹ã®åé›†
    all_classes = []
    for tag in soup.find_all(class_=True):
        if isinstance(tag['class'], list):
            all_classes.extend(tag['class'])
        else:
            all_classes.append(tag['class'])
    analysis['css_classes'] = list(set(all_classes))[:20]  # æœ€åˆã®20å€‹
    
    # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ã®ç¢ºèª
    inline_styles = []
    for tag in soup.find_all(style=True):
        inline_styles.append(tag['style'][:100])  # æœ€åˆã®100æ–‡å­—
    analysis['inline_styles'] = inline_styles[:5]  # æœ€åˆã®5ã¤
    
    # ãã®ä»–ã®è¦ç´ ã‚«ã‚¦ãƒ³ãƒˆ
    analysis['links_count'] = len(soup.find_all('a'))
    analysis['images_count'] = len(soup.find_all('img'))
    analysis['tables_count'] = len(soup.find_all('table'))
    analysis['lists_count'] = len(soup.find_all(['ul', 'ol']))
    analysis['divs_count'] = len(soup.find_all('div'))
    analysis['sections_count'] = len(soup.find_all('section'))
    
    # Schema.orgã®ç¢ºèª
    if soup.find(attrs={'itemtype': True}) or soup.find(attrs={'itemscope': True}):
        analysis['has_schema'] = True
    
    # SEOå•é¡Œç‚¹ã®æ¤œå‡º
    if not analysis['heading_structure']:
        analysis['seo_issues'].append('è¦‹å‡ºã—ã‚¿ã‚°(H1-H6)ãŒå­˜åœ¨ã—ãªã„')
    elif not any(h['level'] == 2 for h in analysis['heading_structure']):
        analysis['seo_issues'].append('H2ã‚¿ã‚°ãŒå­˜åœ¨ã—ãªã„ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹é€ ãŒä¸æ˜ç¢ºï¼‰')
    
    if not analysis['has_semantic_tags']:
        analysis['seo_issues'].append('ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯HTML5ã‚¿ã‚°ãŒä½¿ç”¨ã•ã‚Œã¦ã„ãªã„')
    
    if not analysis['has_schema']:
        analysis['seo_issues'].append('Schema.orgæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„')
    
    if analysis['inline_styles']:
        analysis['seo_issues'].append(f'ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ãŒ{len(inline_styles)}ç®‡æ‰€ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ï¼ˆCSSåˆ†é›¢æ¨å¥¨ï¼‰')
    
    if analysis['divs_count'] > 10 and not analysis['has_semantic_tags']:
        analysis['seo_issues'].append('divã‚¿ã‚°ãŒå¤šç”¨ã•ã‚Œã¦ã„ã‚‹ãŒã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¿ã‚°ãŒæœªä½¿ç”¨')
    
    return analysis

def main():
    print("=" * 80)
    print("åŠ©æˆé‡‘ãƒ»è£œåŠ©é‡‘æŠ•ç¨¿ Cåˆ—æœ¬æ–‡ SEOåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 80)
    print()
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
    wb = openpyxl.load_workbook('google_sheets_export.xlsx', read_only=True, data_only=True)
    ws = wb.active
    
    print(f"ğŸ“Š ã‚·ãƒ¼ãƒˆå: {ws.title}")
    print(f"ğŸ“Š ç·è¡Œæ•°: {ws.max_row}")
    print()
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®ç¢ºèª
    headers = [cell.value for cell in ws[1]]
    print("ğŸ“‹ ã‚«ãƒ©ãƒ æ§‹é€ :")
    for i, header in enumerate(headers[:10], 1):  # æœ€åˆã®10åˆ—
        print(f"  {chr(64+i)}åˆ—: {header}")
    print()
    
    # Cåˆ—ï¼ˆæœ¬æ–‡ï¼‰ã‚’åˆ†æ
    print("ğŸ” Cåˆ—ï¼ˆæœ¬æ–‡ï¼‰ã®è©³ç´°åˆ†æã‚’é–‹å§‹...")
    print()
    
    all_analyses = []
    sample_contents = []
    
    # 2è¡Œç›®ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿è¡Œ
    max_row = ws.max_row if ws.max_row else 1000  # max_rowãŒNoneã®å ´åˆã¯1000è¡Œã¾ã§
    for row_num, row in enumerate(ws.iter_rows(min_row=2, max_row=min(52, max_row)), start=2):
        post_id = row[0].value if row[0].value else f"Row{row_num}"
        title = row[1].value if len(row) > 1 and row[1].value else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
        content = row[2].value if len(row) > 2 else None
        
        analysis = analyze_html_structure(content)
        if analysis:
            analysis['post_id'] = post_id
            analysis['title'] = title[:50]
            all_analyses.append(analysis)
            
            # æœ€åˆã®3ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦ä¿å­˜
            if len(sample_contents) < 3 and content:
                sample_contents.append({
                    'post_id': post_id,
                    'title': title,
                    'content_preview': content[:1000] if content else ""
                })
    
    print(f"âœ… åˆ†æå®Œäº†: {len(all_analyses)}ä»¶ã®æŠ•ç¨¿ã‚’åˆ†æ")
    print()
    
    # çµ±è¨ˆæƒ…å ±
    print("=" * 80)
    print("ğŸ“Š çµ±è¨ˆã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    print()
    
    total = len(all_analyses)
    has_html = sum(1 for a in all_analyses if a['has_html_tags'])
    has_semantic = sum(1 for a in all_analyses if a['has_semantic_tags'])
    has_schema = sum(1 for a in all_analyses if a['has_schema'])
    has_headings = sum(1 for a in all_analyses if a['heading_structure'])
    has_inline_styles = sum(1 for a in all_analyses if a['inline_styles'])
    
    print(f"ç·æŠ•ç¨¿æ•°: {total}")
    print(f"HTMLã‚¿ã‚°ã‚ã‚Š: {has_html} ({has_html/total*100:.1f}%)")
    print(f"ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯HTML5ä½¿ç”¨: {has_semantic} ({has_semantic/total*100:.1f}%)")
    print(f"Schema.orgä½¿ç”¨: {has_schema} ({has_schema/total*100:.1f}%)")
    print(f"è¦‹å‡ºã—ã‚¿ã‚°ä½¿ç”¨: {has_headings} ({has_headings/total*100:.1f}%)")
    print(f"ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ä½¿ç”¨: {has_inline_styles} ({has_inline_styles/total*100:.1f}%)")
    print()
    
    # è¦‹å‡ºã—æ§‹é€ ã®çµ±è¨ˆ
    print("=" * 80)
    print("ğŸ“ è¦‹å‡ºã—æ§‹é€ ã®åˆ†æ")
    print("=" * 80)
    print()
    
    heading_usage = Counter()
    for analysis in all_analyses:
        for h in analysis['heading_structure']:
            heading_usage[f"H{h['level']}"] += 1
    
    if heading_usage:
        print("è¦‹å‡ºã—ã‚¿ã‚°ã®ä½¿ç”¨é »åº¦:")
        for tag, count in sorted(heading_usage.items()):
            print(f"  {tag}: {count}ä»¶ ({count/total*100:.1f}%)")
    else:
        print("âš ï¸ è¦‹å‡ºã—ã‚¿ã‚°ãŒå…¨ãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print()
    
    # CSSã‚¯ãƒ©ã‚¹ã®çµ±è¨ˆ
    print("=" * 80)
    print("ğŸ¨ CSSã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨çŠ¶æ³")
    print("=" * 80)
    print()
    
    all_classes = []
    for analysis in all_analyses:
        all_classes.extend(analysis['css_classes'])
    
    if all_classes:
        class_counter = Counter(all_classes)
        print(f"ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹CSSã‚¯ãƒ©ã‚¹ç·æ•°: {len(set(all_classes))}")
        print("\né »å‡ºCSSã‚¯ãƒ©ã‚¹ Top 20:")
        for cls, count in class_counter.most_common(20):
            print(f"  .{cls}: {count}å›")
    else:
        print("âš ï¸ CSSã‚¯ãƒ©ã‚¹ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print()
    
    # SEOå•é¡Œç‚¹ã®é›†è¨ˆ
    print("=" * 80)
    print("âš ï¸ SEOå•é¡Œç‚¹ã®é›†è¨ˆ")
    print("=" * 80)
    print()
    
    all_issues = []
    for analysis in all_analyses:
        all_issues.extend(analysis['seo_issues'])
    
    if all_issues:
        issue_counter = Counter(all_issues)
        print(f"æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ç·æ•°: {len(all_issues)}")
        print("\né »å‡ºå•é¡Œ Top 10:")
        for issue, count in issue_counter.most_common(10):
            print(f"  â€¢ {issue}: {count}ä»¶ ({count/total*100:.1f}%)")
    else:
        print("âœ… é‡å¤§ãªSEOå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    print()
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¡¨ç¤º
    print("=" * 80)
    print("ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆæœ€åˆã®3ä»¶ï¼‰")
    print("=" * 80)
    print()
    
    for i, sample in enumerate(sample_contents, 1):
        print(f"\n--- ã‚µãƒ³ãƒ—ãƒ« {i} ---")
        print(f"æŠ•ç¨¿ID: {sample['post_id']}")
        print(f"ã‚¿ã‚¤ãƒˆãƒ«: {sample['title']}")
        print(f"\næœ¬æ–‡ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰:")
        print("-" * 80)
        print(sample['content_preview'])
        print("-" * 80)
        print()
    
    # è©³ç´°åˆ†æçµæœã‚’JSONã§ä¿å­˜
    print("=" * 80)
    print("ğŸ’¾ è©³ç´°åˆ†æçµæœã‚’ä¿å­˜ä¸­...")
    print("=" * 80)
    print()
    
    with open('content_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump({
            'summary': {
                'total_posts': total,
                'has_html_tags': has_html,
                'has_semantic_tags': has_semantic,
                'has_schema': has_schema,
                'has_headings': has_headings,
                'has_inline_styles': has_inline_styles
            },
            'heading_usage': dict(heading_usage),
            'common_classes': dict(class_counter.most_common(50)) if all_classes else {},
            'common_issues': dict(issue_counter.most_common(20)) if all_issues else {},
            'detailed_analyses': all_analyses[:20]  # æœ€åˆã®20ä»¶ã®è©³ç´°
        }, f, ensure_ascii=False, indent=2)
    
    print("âœ… è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ content_analysis_report.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print()
    
    # æ¨å¥¨ã•ã‚Œã‚‹æ”¹å–„ç­–
    print("=" * 80)
    print("ğŸ’¡ æ¨å¥¨ã•ã‚Œã‚‹æ”¹å–„ç­–")
    print("=" * 80)
    print()
    
    print("1. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯HTML5æ§‹é€ ã®å°å…¥")
    print("   â€¢ <article> ã‚¿ã‚°ã§æŠ•ç¨¿å…¨ä½“ã‚’ãƒ©ãƒƒãƒ—")
    print("   â€¢ <section> ã‚¿ã‚°ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†å‰²")
    print("   â€¢ <header> ã‚¿ã‚°ã§ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—")
    print()
    
    print("2. è¦‹å‡ºã—éšå±¤ã®çµ±ä¸€")
    print("   â€¢ H1: æŠ•ç¨¿ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ1ã¤ã®ã¿ï¼‰")
    print("   â€¢ H2: ãƒ¡ã‚¤ãƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå¯¾è±¡è€…ã€å†…å®¹ã€ç”³è«‹æ–¹æ³•ãªã©ï¼‰")
    print("   â€¢ H3: ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
    print()
    
    print("3. Schema.orgæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ")
    print("   â€¢ GovernmentService ã¾ãŸã¯ Grant ã‚¹ã‚­ãƒ¼ãƒã®å®Ÿè£…")
    print("   â€¢ JSON-LDå½¢å¼ã§ã®åŸ‹ã‚è¾¼ã¿")
    print()
    
    print("4. CSSè¨­è¨ˆã®çµ±ä¸€")
    print("   â€¢ BEMå‘½åè¦å‰‡ã®æ¡ç”¨ï¼ˆ.grant-[block]__[element]--[modifier]ï¼‰")
    print("   â€¢ ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ã®æ’é™¤")
    print("   â€¢ ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³ã®å®Ÿè£…")
    print()
    
    print("5. ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®å‘ä¸Š")
    print("   â€¢ ARIAå±æ€§ã®é©åˆ‡ãªä½¿ç”¨")
    print("   â€¢ altå±æ€§ã®è¨­å®š")
    print("   â€¢ ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ç®¡ç†ã®æ”¹å–„")
    print()
    
    wb.close()
    print("=" * 80)
    print("åˆ†æå®Œäº†ï¼")
    print("=" * 80)

if __name__ == '__main__':
    main()
